<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Tong Zhang</title>

    <meta name="author" content="Tong Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Tong Zhang
                </p>
                <p>I am a Ph.D. student at <a href="https://iiis.tsinghua.edu.cn/en/"> Institute for Interdisciplinary Information Sciences (IIIS), <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University</a>, advised by Prof. <a href="https://yang-gao.weebly.com/">Yang Gao</a>. Previously, I received my bachelor degree from <a href="https://www.ee.tsinghua.edu.cn/en/">Department of Electronic Engineering</a> in Tsinghua University.
                <p>
                  My primary research interest lies at the intersection of computer vision and robotics. I am particularly focused on the application of 3D vision in robotic perception and am committed to developing universal and real-world effective perception modules for robots.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zhangton20@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BsMsRuIAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/TongZhangTHU">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:25%;max-width:30%">
                <!-- <a href="images/TongZhang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/TongZhang.jpg" class="hoverZoomLink"></a> -->
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/TongZhang.jpg"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                
      </tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video src="images/vila.mp4" width="250" height="250" autoplay muted loop>
          </video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://robot-vila.github.io/">
            <span class="papertitle">Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning</span>
          </a>
          <br>
        
          <a href="https://yingdong-hu.github.io/">Yingdong Hu*</a>, <a href="https://scholar.google.com/citations?user=IhZxOR4AAAAJ&hl=en&oi=ao">Fanqi Lin*</a>, <strong>Tong Zhang</strong>,  <a href="https://ericyi.github.io/">Li Yi</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
          <br>
          <em>arXiv</em>, 2023
          <br>
          <a href="https://robot-vila.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2311.17842">arXiv</a>
          <p></p>
          <p>We introduce ViLa, a novel approach for long-horizon robotic planning that leverages GPT-4V to generate a sequence of actionable steps. ViLa empowers robots to execute complex tasks with a profound understanding of the visual world.</p>
        </td>
      </tr>

      </tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/sgr.jpeg" width="283" height="115">
          <!-- <img src="images/sgr.jpeg" alt="clean-usnob" width="283" height="115"> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://semantic-geometric-representation.github.io/">
            <span class="papertitle">A Universal Semantic-Geometric Representation for Robotic Manipulation</span>
          </a>
          <br>
        
          <strong>Tong Zhang*</strong>, <a href="https://yingdong-hu.github.io/">Yingdong Hu*</a>,  Hanchen Cui</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
          <br>
          <em>CoRL</em>, 2023
          <br>
          <a href="https://semantic-geometric-representation.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2306.10474">arXiv</a>
          <p></p>
          <p>We present <strong>Semantic-Geometric Representation (SGR)</strong>, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning.</p>
        </td>
      </tr>
  
    </table>
  </body>
</html>
