<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Tong Zhang</title>

    <meta name="author" content="Tong Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Tong Zhang  Âº†ÂΩ§
                </p>
                <p>I am a Ph.D. student at <a href="https://iiis.tsinghua.edu.cn/en/"> Institute for Interdisciplinary Information Sciences</a> (IIIS, headed by Turing award winner Prof. <a href="https://iiis.tsinghua.edu.cn/yao/">Andrew Chi-Chih Yao</a>), <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University</a>. I am fortunate to be advised by Prof. <a href="https://yang-gao.weebly.com/">Yang Gao</a>. Previously, I received my bachelor's degree from <a href="https://www.ee.tsinghua.edu.cn/en/">Department of Electronic Engineering</a> at Tsinghua University.
                <p>
                  I am currently a visiting scholar at <a href="https://www.berkeley.edu/">UC Berkeley</a>, advised by <a href="https://me.berkeley.edu/people/koushil-sreenath/">Koushil Sreenath</a> in the <a href="https://hybrid-robotics.berkeley.edu/">Hybrid Robotics Group (HRG)</a> and <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research Lab (BAIR)</a>. 
                <p>
                  My primary research interest is in Embodied AI, which lies at the intersection of Artificial Intelligence and Robotics. I focus on topics such as robotic manipulation, humanoid robots, whole-body control, and representation learning. Recently, I am particularly interested in humanoid loco-manipulation.
                <p>
                  <strong>Email:</strong> zhangton20 [AT] mails.tsinghua.edu.cn
                <p>
                  <span style="color: red;">I expect to graduate in the summer of 2026 and will look for suitable job opportunities.  ÊàëÈ¢ÑËÆ°Â∞ÜÂú®2026Âπ¥Â§èÂ§©ÊØï‰∏öÔºåÂπ∂Â∞ÜÂØªÊ±ÇÂêàÈÄÇÁöÑÂ∑•‰ΩúÊú∫‰ºö„ÄÇ</span>
                <p>
                </p>
                <p style="text-align:center">
                  <a href="mailto:zhangton20@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BsMsRuIAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/TongZhangTHU">Github</a> &nbsp;/&nbsp;
                  <a href="images/wechat.jpg" target="_blank">WeChat (ÂæÆ‰ø°)</a>
                </p>
              </td>
              <td style="padding:2.5%;width:25%;max-width:30%">
                <!-- <a href="images/TongZhang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/TongZhang.jpg" class="hoverZoomLink"></a> -->
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/TongZhang.jpg"></a>
              </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
              <h2 style="margin-bottom: 20px;">News</h2>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2025.08] One paper (<a href="https://hub-robot.github.io/">HuB</a>) is accepted at <a href="https://www.corl.org/">CoRL 2025</a> <span style="color: red;">(Oral Presentation)</span>.</li>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2025.06] One paper (<a href="https://hub-robot.github.io/">HuB</a>) is accepted at <a href="https://wcbm-workshop.github.io/">RSS Workshop on Whole-body Control and Bimanual Manipulation, 2025</a>.</li>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2024.09] Three papers (<a href="https://sgrv2-robot.github.io/">SGRv2</a>, <a href="https://openreview.net/forum?id=dsxmR6lYlg">RLFP</a>, and <a href="https://general-flow.github.io/">General Flow</a>) are accepted at <a href="https://2024.corl.org/">CoRL 2024</a>.</li>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2023.12] Invited oral presentation at <a href="http://www.adai.ai/dai/2023/invited_papers.html">DAI 2023</a>.</li>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2023.10] Invited talk at <a href="http://rlchina.org/topic/803">RLChina</a>.</li>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2023.08] One paper (<a href="https://semantic-geometric-representation.github.io/">SGR</a>) is accepted at <a href="https://www.corl2023.org/">CoRL 2023</a>.</li>
              <li style="margin-bottom: 5px; text-indent: 20px;">[2023.04] One paper (<a href="https://drive.google.com/file/d/1Lj9ib3vuTJ6vZnl7sQphcqKUhkEGJL_j/view">SGN</a>) is accepted at <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics"> CVPR 2023 Workshop on 3D Vision and Robotics</a>.</li>
            </td>
          </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          </tr>
          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
            <video src="images/hub_homepage2.mp4" width="300" height="300" autoplay muted loop>
            </video>
          </td> -->
          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
            <div style="width:300px; height:300px; border-radius:15px; overflow:hidden; position:relative;">
              <video src="images/hub_homepage2.mp4"
                     autoplay
                     muted
                     loop
                     playsinline
                     style="width:100%; height:100%; object-fit:cover; display:block;">
              </video>
            </div>
          </td> -->
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div style="width:300px; height:200px; border-radius:15px; overflow:hidden; position:relative;">
              <video src="images/hub_homepage_compressed.mp4"
                     autoplay
                     muted
                     loop
                     playsinline
                     style="position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover;">
              </video>
            </div>
          </td>
          
          
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://hub-robot.github.io/">
              <span class="papertitle">HuB: Learning Extreme Humanoid Balance</span>
            </a>
            <br>
          
            <strong>Tong Zhang*</strong>,  
            <a href="https://github.com/ZhengBryan">Boyuan Zheng*</a>, 
            <a href="https://scholar.google.com/citations?user=I0HLZAwAAAAJ&hl=en">Ruiqian Nai</a>, 
            <a href="https://yingdong-hu.github.io/">Yingdong Hu</a>, 
            <a href="https://wangyenjen.github.io/">Yen-Jen Wang</a>, 
            <a href="https://jc043.github.io/">Geng Chen</a>, 
            <a href="https://fanqi-lin.github.io/">Fanqi Lin</a>, 
            <a href="https://github.com/Cata1ysttt">Jiongye Li</a>, 
            <a href="https://chuye03.github.io/">Chuye Hong</a>, 
            <a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath</a>, 
            <a href="https://yang-gao.weebly.com/">Yang Gao</a>
            <br>
            <em>CoRL</em>, 2025 <span style="color: red;">(Oral Presentation)</span>
            <br>
            <em>RSS Workshop on Whole-body Control and Bimanual Manipulation</em>, 2025
            <br>
            <a href="https://hub-robot.github.io/">project page</a> / 
            <a href="https://arxiv.org/abs/2505.07294">arXiv</a> /
            <a href="https://x.com/TongZha22057330/status/1922092674644894041">X summary</a>
            <p></p>
            <p>We propose <strong>HuB</strong> (<strong>Hu</strong>manoid <strong>B</strong>alance), a framework that enables humanoids to perform challenging quasi-static balance tasks, including extreme single-legged poses such as the Swallow Balance and Bruce Lee's Kick. </p>
          </td>
        </tr>


        </tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <!-- <img src="images/sgrv2.jpeg" width="283" height="115"> -->
            <img src="images/sgrv2_left.jpeg" width="120" height="100" style="display:block;margin:0 auto;">
            <img src="images/sgrv2_right.jpeg" width="283" height="150">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://sgrv2-robot.github.io/">
              <span class="papertitle">Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation</span>
            </a>
            <br>
          
            <strong>Tong Zhang</strong>, <a href="https://yingdong-hu.github.io/">Yingdong Hu</a>,  Jiacheng You</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
            <br>
            <em>CoRL</em>, 2024
            <br>
            <a href="https://sgrv2-robot.github.io/">project page</a> / 
            <a href="https://arxiv.org/abs/2406.10615">arXiv</a> /
            <a href="https://github.com/TongZhangTHU/sgr">code</a> /
            <a href="https://x.com/TongZha22057330/status/1831559156463177858">X summary</a>
            <p></p>
            <p>We introduce <strong>SGRv2</strong>, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias-<strong>action locality</strong>, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment.</p>
          </td>
        </tr>

      </tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/RLPF.png" width="283" height="160">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://yewr.github.io/rlfp/">
          <span class="papertitle">Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own</span>
        </a>
        <br>
        <a href="https://yewr.github.io/">Weirui Ye</a>, Yunsheng Zhang, Haoyang Weng, <a href="https://xianfangu.github.io/">Xianfan Gu</a>, <a href="https://shengjie-bob.github.io/">Shengjie Wang</a>, <strong>Tong Zhang</strong>, Mengchen Wang, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
        <br>
        <em>CoRL</em>, 2024 <span style="color: red;">(Oral Presentation)</span>
        <br>
        <a href="https://yewr.github.io/rlfp/">project page</a> / 
        <a href="https://arxiv.org/abs/2310.02635">arXiv</a> /
        <a href="https://github.com/YeWR/RLFP">code</a>
        <br>
        <p></p>
        <p>We propose Reinforcement Learning with Foundation Priors (<strong>RLFP</strong>) to utilize guidance and feedback from policy, value, and success-reward foundation models. Within this framework, we introduce the Foundation-guided Actor-Critic (FAC) algorithm, which enables embodied agents to explore more efficiently with automatic reward functions.</p>
      </td>
    </tr>

      </tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video src="images/general_flow.mp4" width="250" height="250" autoplay muted loop>
          </video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://general-flow.github.io/">
            <span class="papertitle">General Flow as Foundation Affordance for Scalable Robot Learning</span>
          </a>
          <br>
        
          <a href="https://michaelyuancb.github.io/">Chengbo Yuan</a>, <a href="https://alvinwen428.github.io/">Chuan Wen</a>, <strong>Tong Zhang</strong>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
          <br>
          <em>CoRL</em>, 2024
          <br>
          <a href="https://general-flow.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2401.11439">arXiv</a> /
          <a href="https://github.com/michaelyuancb/general_flow">code</a>
          <p></p>
          <p>We build a 3D flow prediction model directly from large-scale RGBD human video datasets. Based on this model, we achieve stable zero-shot human-to-robot skill transfer in the real world.</p>
        </td>
      </tr>
    


      </tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <video src="images/vila.mp4" width="250" height="250" autoplay muted loop>
          </video>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://robot-vila.github.io/">
            <span class="papertitle">Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning</span>
          </a>
          <br>
        
          <a href="https://yingdong-hu.github.io/">Yingdong Hu*</a>, <a href="https://scholar.google.com/citations?user=IhZxOR4AAAAJ&hl=en&oi=ao">Fanqi Lin*</a>, <strong>Tong Zhang</strong>,  <a href="https://ericyi.github.io/">Li Yi</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
          <br>
          <em>ICRA Workshop on Vision-Language Models for Navigation and Manipulation</em>, 2024
          <br>
          <a href="https://robot-vila.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2311.17842">arXiv</a>
          <p></p>
          <p>We introduce <strong>ViLa</strong>, a novel approach for long-horizon robotic planning that leverages GPT-4V to generate a sequence of actionable steps. ViLa empowers robots to execute complex tasks with a profound understanding of the visual world.</p>
        </td>
      </tr>

      </tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/sgr.jpeg" width="283" height="115">
          <!-- <img src="images/sgr.jpeg" alt="clean-usnob" width="283" height="115"> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://semantic-geometric-representation.github.io/">
            <span class="papertitle">A Universal Semantic-Geometric Representation for Robotic Manipulation</span>
          </a>
          <br>
        
          <strong>Tong Zhang*</strong>, <a href="https://yingdong-hu.github.io/">Yingdong Hu*</a>,  Hanchen Cui</a>, <a href="https://hangzhaomit.github.io/">Hang Zhao</a>, <a href="https://yang-gao.weebly.com/">Yang Gao</a>
          <br>
          <em>CoRL</em>, 2023
          <br>
          <em>CVPR Workshop on 3D Vision and Robotics</em>, 2023
          <br>
          <a href="https://semantic-geometric-representation.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2306.10474">arXiv</a> /
          <a href="https://github.com/TongZhangTHU/sgr">code</a>
          <p></p>
          <p>We present <strong>Semantic-Geometric Representation (SGR)</strong>, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning.</p>
        </td>
      </tr>
  
    </table>
  </body>
  <!-- <a href="https://hits.seeyoufarm.com">
    <img style="display: block; margin-left: auto; margin-right: auto;" 
         src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Ftongzhangthu.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/>
  </a> -->
</html>
